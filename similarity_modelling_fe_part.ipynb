{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62EpXITuiK0D"
      },
      "source": [
        "# 0. Introduction\n",
        "\n",
        "Student: Marko Kadic, 12045128\n",
        "Contribution as well: Billie Postlmayrl, 01307120\n",
        "\n",
        "The goal of this notebook is to build a good classifier, the task of which is to recognize cahracters in the Muppet Show. <br>\n",
        "**Kermit the Frog** and **Waldorf & Statler** are the characters I am aiming to recognize. <br>\n",
        "\n",
        "Furthermore, the methods used will be those of a classic ML classification, paired with **Feature Engineering** methods. <br>\n",
        "\n",
        "The features will be of both audio and video domain. <br>\n",
        "\n",
        "**Dependency installations:** <br>\n",
        "pip install numpy <br>\n",
        "pip install pandas <br>\n",
        "pip install opencv-python <br>\n",
        "pip install tensorflow <br>\n",
        "pip install librosa <br>\n",
        "pip install matplotlib <br>\n",
        "pip install scipy <br>\n",
        "!pip install  opencv-python==3.4.2.17 <br>\n",
        "!pip install  opencv-contrib-python==3.4.2.17 <br>\n",
        "\n",
        "**Complete Timesheet:** <br>\n",
        "Installation and notebook setup: 25th November, 0.5 hours <br>\n",
        "Data preprocessing: 25th November, 3 hours <br>\n",
        "Watched Lecture 3/10 - Feature Engineering: 27th November, 1.5 hours <br>\n",
        "Watched Lecture 7/10 -  Integral Transforms & Spectral Features: 28th November, 2 hours <br>\n",
        "After watching the lectures, I have decided on which features to extract from the data: 28th November, 0.5 hours <br>\n",
        "Effort to understand multiple audio signal processing methods: 4th December, 3 hours <br>\n",
        "Extraction of Time Domain Audio Features: 5th December, 2 hours <br>\n",
        "Extraction of Time Domain Audio Features: 6th December, 2 hours <br>\n",
        "Watched Lecture 1/10 - 1.5 hrs  Jan 3rd<br>\n",
        "Watched Lecture 2/10 - 1.5 hrs Jan 3d<br> \n",
        "Watched Lecture 4/10 - 1.5 hrs Jan 3rd<br> \n",
        "Watched Lecture 5/10 - 1.5 hrs Jan 3rd<br> \n",
        "Watched Lecture 9/10 - 0.5 hrs Jan 3rd<br> \n",
        "Additional Work on Time Domain Audio Features: 2 hours Jan 4th<br> \n",
        "Additional Work on Frequency Domain Audio Features: 0.5 hours Jan 4th <br> \n",
        "Setting up Google Drive Environment: 0.5 hours Jan 4th <br>\n",
        "Exploration and Learning about Image Features: 5 hours Jan 4th <br> \n",
        "Extraction of Time Domain Image Features 1: 7 hours Jan 5th<br> \n",
        "Extraction of Time Domain Image Features 2: 6 hours  Jan 5th<br>\n",
        "Extraction of Frequency Domain Image Features: 7 hours Jan 6th<br> \n",
        "Extraction of Local Image Features 1 (HOG): 10 hours  Jan 6th/7th<br>\n",
        "Extraction of Local Image Features 2 (SIFT-bag): 8 hours  Jan <br>7th-10th\n",
        "Extraction of Local Image Features 3 (Corners, SIFT): 10 hours Jan 7th-10th<br>\n",
        "Feature Calculations/Calculation Time: 15 hours Jan 7th-10th <br>\n",
        "Preparation of Features for Classification: 5 hours Jan 7th-10th<br>\n",
        "Creation of Classifiers: 2 hours Jan 7th-10th <br>\n",
        "Classification: 10 hours Jan 7th-10th <br>\n",
        "Evaluation: 0.5 hours Jan 7th-10th<br>\n",
        "Comments on Methods and Feature Qualities: 0.5 hours Jan 7th-10th <br>\n",
        "\n",
        "\n",
        "Time Billie Postlmayrl (lab partner) 01307120: <br>\n",
        "Extraction of Image features - Color Features: 5 hours <br>\n",
        "Extraction of Audio features - Time Domain Audio: 6 hours <br>\n"
      ],
      "id": "62EpXITuiK0D"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PVPFBFcliK0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b5398f-3ea8-4eaa-f134-7f7b7b4aa15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install  opencv-python==3.4.2.17\n",
        "!pip install  opencv-contrib-python==3.4.2.17\n",
        "import cv2 \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "from skimage import feature\n",
        "import scipy.spatial.distance\n",
        "import scipy.cluster\n",
        "import scipy\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from scipy.fftpack import dct"
      ],
      "id": "PVPFBFcliK0J"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlta8pWVl9cB",
        "outputId": "bf83e9f0-2ed6-4a95-cd62-a77b15abe093"
      },
      "id": "wlta8pWVl9cB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEy7z8WbnIQW",
        "outputId": "6e5d62e1-0652-4342-dc79-f106f01ccb42"
      },
      "id": "lEy7z8WbnIQW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sim12-ground-truth-muppets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuKhYJo6iK0M"
      },
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "I have extracted the audio files as separate files.\n",
        "I have converted the original videos using cv2 library, into arrays of frames.\n",
        "\n",
        "The plan is to do the audio and video feature engineering separately and later on, combine the solutions.\n",
        "\n",
        "I have loaded up the ground truth annotations and have split them into train and test parts - 2 episodes for training, 1 episode for testing."
      ],
      "id": "WuKhYJo6iK0M"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v12XNbU-iK0N"
      },
      "outputs": [],
      "source": [
        "# PATHS TO FULL EPISODES\n",
        "path_ep1 = \"data/episodes/Muppets-02-01-01.avi\"\n",
        "path_ep2 = \"data/episodes/Muppets-02-04-04.avi\"\n",
        "path_ep3 = \"data/episodes/Muppets-03-04-03.avi\"\n",
        "\n",
        "# GOOGLE COLLAB ADD ON\n",
        "path_ep1 = \"/content/drive/MyDrive/\" + path_ep1\n",
        "path_ep2 = \"/content/drive/MyDrive/\" + path_ep2\n",
        "path_ep3 = \"/content/drive/MyDrive/\" + path_ep3"
      ],
      "id": "v12XNbU-iK0N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkNtBbrIiK0O",
        "outputId": "294ff7c6-9596-4785-edec-fca531003d50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# EXTRACT AUDIO FROM EPISODES\n",
        "#import subprocess\n",
        "\n",
        "#command1 = \"ffmpeg -i \" + path_ep1 + \" -ab 160k -ac 2 -ar 44100 -vn data/episodes/audio_ep1.wav\"\n",
        "#command2 = \"ffmpeg -i \" + path_ep2 + \" -ab 160k -ac 2 -ar 44100 -vn data/episodes/audio_ep2.wav\"\n",
        "#command3 = \"ffmpeg -i \" + path_ep3 + \" -ab 160k -ac 2 -ar 44100 -vn data/episodes/audio_ep3.wav\"\n",
        "\n",
        "#subprocess.call(command1, shell=True)\n",
        "#subprocess.call(command2, shell=True)\n",
        "#subprocess.call(command3, shell=True)"
      ],
      "id": "lkNtBbrIiK0O"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eEd5Pc0liK0Q"
      },
      "outputs": [],
      "source": [
        "# PATHS TO AUDIO FILES\n",
        "audio_ep1 = \"data/episodes/audio_ep1.wav\"\n",
        "audio_ep2 = \"data/episodes/audio_ep2.wav\"\n",
        "audio_ep3 = \"data/episodes/audio_ep3.wav\"\n",
        "\n",
        "# GOOGLE COLLAB ADD ON\n",
        "audio_ep1 = \"/content/drive/MyDrive/\" + audio_ep1\n",
        "audio_ep2 = \"/content/drive/MyDrive/\" + audio_ep2\n",
        "audio_ep3 = \"/content/drive/MyDrive/\" + audio_ep3"
      ],
      "id": "eEd5Pc0liK0Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQIUI0TSiK0R"
      },
      "outputs": [],
      "source": [
        "# EXTRACT FRAMES FROM EPISODES\n",
        "###################################\n",
        "# Google Drive extra\n",
        "gd_loc = \"/content/drive/MyDrive/data/processed_data/episodes/ep1/\"\n",
        "\n",
        "# EPISODE 1\n",
        "vidcap = cv2.VideoCapture(path_ep1)\n",
        "(success,image) = vidcap.read()\n",
        "\n",
        "count = 0\n",
        "row_count = 0\n",
        "\n",
        "(height, width, channels) = image.shape\n",
        "\n",
        "frames_ep1 = []\n",
        "counter = 0\n",
        "batch_counter = 1\n",
        "frame_batch_size = 10000\n",
        "scale_percent = 50 # percent of original size\n",
        "width = int(image.shape[1] * scale_percent / 100)\n",
        "height = int(image.shape[0] * scale_percent / 100)\n",
        "dim = (width, height)\n",
        "\n",
        "# BATCH LOAD THE EPISODE DUE TO RAM LIMITATIONS\n",
        "while success:\n",
        "    if(counter >= frame_batch_size):\n",
        "      frame_array_ep1 = np.array(frames_ep1)\n",
        "      np.save(gd_loc + 'frame_array_ep1_'+ str(batch_counter) +'.npy', frame_array_ep1)\n",
        "      del frame_array_ep1\n",
        "      batch_counter += 1\n",
        "      counter = 0\n",
        "      frames_ep1 = []\n",
        "    else:\n",
        "      counter += 1\n",
        "\n",
        "    # RESIZE IMAGE FOR NOW TO USE LESS SPACE FOR PROCESING\n",
        "    try:\n",
        "      resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "    except:\n",
        "      break\n",
        "    frames_ep1.append(resized)\n",
        "    (sucess,image) = vidcap.read()\n",
        "\n",
        "frame_array_ep1 = np.array(frames_ep1)\n",
        "np.save(gd_loc + 'frame_array_ep1_'+ str(batch_counter) +'.npy', frame_array_ep1)\n",
        "del frame_array_ep1\n",
        "del frames_ep1\n",
        "\n",
        "###################################\n",
        "# EPISODE 2\n",
        "gd_loc = \"/content/drive/MyDrive/data/processed_data/episodes/ep2/\"\n",
        "\n",
        "vidcap = cv2.VideoCapture(path_ep2)\n",
        "(success,image) = vidcap.read()\n",
        "\n",
        "count = 0\n",
        "row_count = 0\n",
        "\n",
        "frames_ep2 = []\n",
        "counter = 0\n",
        "batch_counter = 1\n",
        "\n",
        "# BATCH LOAD THE EPISODE DUE TO RAM LIMITATIONS\n",
        "while success:\n",
        "    if(counter >= frame_batch_size):\n",
        "      frame_array_ep2 = np.array(frames_ep2)\n",
        "      np.save(gd_loc + 'frame_array_ep2_'+ str(batch_counter) +'.npy', frame_array_ep2)\n",
        "      del frame_array_ep2\n",
        "      batch_counter += 1\n",
        "      counter = 0\n",
        "      frames_ep2 = []\n",
        "    else:\n",
        "      counter += 1\n",
        "\n",
        "    # RESIZE IMAGE FOR NOW TO USE LESS SPACE FOR PROCESING\n",
        "    try:\n",
        "      resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "    except:\n",
        "      break\n",
        "    frames_ep2.append(resized)\n",
        "    (sucess,image) = vidcap.read()\n",
        "\n",
        "frame_array_ep2 = np.array(frames_ep2)\n",
        "np.save(gd_loc + 'frame_array_ep2_'+ str(batch_counter) +'.npy', frame_array_ep2)\n",
        "del frame_array_ep2\n",
        "del frames_ep2\n",
        "\n",
        "\n",
        "###################################    \n",
        "# EPISODE 3\n",
        "gd_loc = \"/content/drive/MyDrive/data/processed_data/episodes/ep3/\"\n",
        "\n",
        "vidcap = cv2.VideoCapture(path_ep3)\n",
        "(success,image) = vidcap.read()\n",
        "\n",
        "count = 0\n",
        "row_count = 0\n",
        "\n",
        "frames_ep3 = []\n",
        "counter = 0\n",
        "batch_counter = 1\n",
        "\n",
        "# BATCH LOAD THE EPISODE DUE TO RAM LIMITATIONS\n",
        "while success:\n",
        "    if(counter >= frame_batch_size):\n",
        "      frame_array_ep3 = np.array(frames_ep3)\n",
        "      np.save(gd_loc + 'frame_array_ep3_'+ str(batch_counter) +'.npy', frame_array_ep3)\n",
        "      del frame_array_ep3\n",
        "      batch_counter += 1\n",
        "      counter = 0\n",
        "      frames_ep3 = []\n",
        "    else:\n",
        "      counter += 1\n",
        "\n",
        "    # RESIZE IMAGE FOR NOW TO USE LESS SPACE FOR PROCESING\n",
        "    try:\n",
        "      resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "    except:\n",
        "      break\n",
        "    frames_ep3.append(resized)\n",
        "    (sucess,image) = vidcap.read()\n",
        "\n",
        "frame_array_ep3 = np.array(frames_ep3)\n",
        "np.save(gd_loc + 'frame_array_ep3_'+ str(batch_counter) +'.npy', frame_array_ep3)\n",
        "del frame_array_ep3\n",
        "del frames_ep3"
      ],
      "id": "rQIUI0TSiK0R"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tt2TgWi7iK0T"
      },
      "outputs": [],
      "source": [
        "# EXTRACT TRAINING AND TESTING ANNOTTATIONS\n",
        "ground_truth_ep1 = \"data/sim12-ground-truth-muppets/GroundTruth_Muppets-02-01-01.csv\"\n",
        "ground_truth_ep2 = \"data/sim12-ground-truth-muppets/GroundTruth_Muppets-02-04-04.csv\"\n",
        "ground_truth_ep3 = \"data/sim12-ground-truth-muppets/GroundTruth_Muppets-03-04-03.csv\"\n",
        "\n",
        "\n",
        "# Google collab addon\n",
        "ground_truth_ep1 = \"/content/drive/MyDrive/\" + ground_truth_ep1\n",
        "ground_truth_ep2 = \"/content/drive/MyDrive/\" + ground_truth_ep2\n",
        "ground_truth_ep3 = \"/content/drive/MyDrive/\" + ground_truth_ep3\n",
        "\n",
        "annotations_train = pd.concat([pd.read_csv(ground_truth_ep1, sep=';'), pd.read_csv(ground_truth_ep2, sep=';')]).sample(frac=1)\n",
        "annotations_test = pd.read_csv(ground_truth_ep3, sep=';')\n",
        "\n",
        "kermit_train  = annotations_train['Kermit']\n",
        "kermit_test = annotations_test['Kermit']\n",
        "\n",
        "statler_waldorf_train = annotations_train['StatlerWaldorf']\n",
        "statler_waldorf_test = annotations_test['StatlerWaldorf']\n",
        "\n",
        "statler_waldorf_audio_train = annotations_train['Audio_StatlerWaldorf']\n",
        "statler_waldorf_audio_test = annotations_test['Audio_StatlerWaldorf']\n"
      ],
      "id": "Tt2TgWi7iK0T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2G8SWFCiK0T"
      },
      "source": [
        "# 2. Lectures\n",
        "\n",
        "After understanding the assignment I have watched 2 lecture videos on the topic of Feature Engineering.\n",
        "\n",
        "1. Lecture 3/10 - Feature Engineering\n",
        "**time / effort:** *27th November, 1.5 hours*\n",
        "\n",
        "2. Lecture 7/10 -  Integral Transforms & Spectral Features\n",
        "**time / effort:** *28th November, 2 hours*\n",
        "\n",
        "3. Lecture 1/10 - Intoduction to Similarity Modelling\n",
        "\n",
        "4. Lecture 2/10 - Similarity\n",
        "\n",
        "5. Lecture 4/10 - Classification\n",
        "\n",
        "6. Lecture 5/10 - Evaluation\n",
        "\n",
        "After watching the lectures, I have decided on which features to extract from the data:\n",
        "**time / effort:** *28th November, 0.5 hours*\n",
        "\n",
        "**Audio:** <br>\n",
        "\n",
        "**Time domain features:**<br>\n",
        "- Loudness\n",
        "- Fundamental Frequency (ZCR)\n",
        "- Fundamental Frequency (YIN)\n",
        "<br>\n",
        "\n",
        "**Frequency domain features:**\n",
        "- The mel frequency cepstral coefficients (MFCCs)\n",
        "- Spectral peaks\n",
        "\n",
        "**Video:** <br>\n",
        "**Time domain features:** <br>\n",
        "- Edge Information (Contour Information)\n",
        "- Edge Orientation Histogram\n",
        "- HUE Information (Dominant colour)\n",
        "- HUE Information - Color Histograms :) <br>\n",
        "\n",
        "**Frequency domain features:** <br>\n",
        "- Luminance + Discrete Cosine Transform<br>\n",
        "\n",
        "**Local methods:**<br>\n",
        "- SIFT - Scale Invariant Feature Transformation (Bag of Visual Features Algorithm)\n",
        "- HOG - Histogram of Oriented Gradients\n",
        "- Shi-Tomasi Corner Detector\n"
      ],
      "id": "O2G8SWFCiK0T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEugkG7liK0U"
      },
      "source": [
        "# Similarity Modelling 1\n",
        "\n",
        "\n",
        "After receving the input from the proffesor the notebook has been split into Similarity Modelling 1 \n",
        "and Similarity Modelling 2 parts:\n",
        "\n",
        "The features relevant for Similarity Modelling 1:\n",
        "\n",
        "**Audio:** <br>\n",
        "Time domain features:\n",
        "- Loudness\n",
        "- Fundamental Frequency (ZCR)\n",
        "- Fundamental Frequency (YIN)\n",
        "\n",
        "**Video:**\n",
        "<br>\n",
        "Time domain features: <br>\n",
        "- Edge Information (Contour Information)\n",
        "- Edge Orientation Histogram\n",
        "- HUE Information (Dominant colour)\n",
        "- HUE Information - Color Histograms :) <br>\n"
      ],
      "id": "vEugkG7liK0U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ppTDFr0iK0V"
      },
      "source": [
        "**A) Time domain Audio** <br>\n",
        "\n",
        "Extraction of previously named Time Domain Audio Features:\n",
        "- Loudness\n",
        "- Fundamental Frequency (ZCR)\n",
        "- Fundamental Frequency (YIN)"
      ],
      "id": "3ppTDFr0iK0V"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tU7aBdcWiK0W"
      },
      "outputs": [],
      "source": [
        "# LOAD UP AUDIO\n",
        "# audio_ep1, audio_ep2, audio_ep3\n",
        "time_audio_loc = \"/content/drive/MyDrive/data/processed_data/time_audio/\""
      ],
      "id": "tU7aBdcWiK0W"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "veOQLJMKiK0Z"
      },
      "outputs": [],
      "source": [
        "# LOUDNESS FEATURE CALCULATION\n",
        "\n",
        "# Root Mean Squared value for signal frame\n",
        "\n",
        "# frame length should change 20ms - 1s\n",
        "\n",
        "rms_ep1_all = []\n",
        "rms_ep2_all = []\n",
        "rms_ep3_all = []\n",
        "signal_ep1, sr  = librosa.load(audio_ep1)\n",
        "for frame_length_iter in range(2048, sr, 2048):\n",
        "  rms_ep1 = librosa.feature.rms(y=signal_ep1, frame_length = int(frame_length_iter))\n",
        "  rms_ep1_all.append(rms_ep1)\n",
        "del signal_ep1\n",
        "\n",
        "signal_ep2, sr  = librosa.load(audio_ep2)\n",
        "for frame_length_iter in range(2048, sr, 2048):\n",
        "  rms_ep2 = librosa.feature.rms(y=signal_ep2, frame_length = int(frame_length_iter))\n",
        "  rms_ep2_all.append(rms_ep2)\n",
        "del signal_ep2\n",
        "\n",
        "signal_ep3, sr  = librosa.load(audio_ep3)\n",
        "for frame_length_iter in range(2048, sr, 2048):\n",
        "  rms_ep3 = librosa.feature.rms(y=signal_ep3, frame_length = int(frame_length_iter))\n",
        "  rms_ep3_all.append(rms_ep3)\n",
        "del signal_ep3\n",
        "\n",
        "np.save(time_audio_loc + 'rms_ep1.npy', rms_ep1_all)\n",
        "np.save(time_audio_loc + 'rms_ep2.npy', rms_ep2_all)\n",
        "np.save(time_audio_loc + 'rms_ep3.npy', rms_ep3_all)"
      ],
      "id": "veOQLJMKiK0Z"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hcBMYm79iK0a"
      },
      "outputs": [],
      "source": [
        "# FUNDAMENTAL FREQUENCY DETECTION BASED ON THE ZERO CROSSINGS RATE\n",
        "signal_ep1, sr  = librosa.load(audio_ep1)\n",
        "signal_ep2, sr  = librosa.load(audio_ep2)\n",
        "signal_ep3, sr  = librosa.load(audio_ep3)\n",
        "\n",
        "zcr_ep1 = librosa.feature.zero_crossing_rate(signal_ep1)\n",
        "zcr_ep2 = librosa.feature.zero_crossing_rate(signal_ep2)\n",
        "zcr_ep3 = librosa.feature.zero_crossing_rate(signal_ep3)\n",
        "\n",
        "np.save(time_audio_loc + 'zcr_ep1.npy', zcr_ep1)\n",
        "np.save(time_audio_loc + 'zcr_ep2.npy', zcr_ep2)\n",
        "np.save(time_audio_loc + 'zcr_ep3.npy', zcr_ep3)"
      ],
      "id": "hcBMYm79iK0a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLynBKy3iK0a"
      },
      "outputs": [],
      "source": [
        "# FUNDAMENTAL FREQUENCY DETECTION BASED ON THE YIN METHOD\n",
        "# We find the fundamental frequency of the audio sample windows\n",
        "signal_ep1, sr  = librosa.load(audio_ep1)\n",
        "signal_ep2, sr  = librosa.load(audio_ep2)\n",
        "signal_ep3, sr  = librosa.load(audio_ep3)\n",
        "\n",
        "f0s_ep1 = librosa.yin(signal_ep1, librosa.note_to_hz('C2'), librosa.note_to_hz('C7')) \n",
        "f0s_ep2 = librosa.yin(signal_ep2, librosa.note_to_hz('C2'), librosa.note_to_hz('C7')) \n",
        "f0s_ep3 = librosa.yin(signal_ep3, librosa.note_to_hz('C2'), librosa.note_to_hz('C7')) \n",
        "\n",
        "np.save(time_audio_loc + 'yin_f0s_ep1.npy', f0s_ep1)\n",
        "np.save(time_audio_loc + 'yin_f0s_ep2.npy', f0s_ep2)\n",
        "np.save(time_audio_loc + 'yin_f0s_ep3.npy', f0s_ep3)"
      ],
      "id": "tLynBKy3iK0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha6DlWeXiK0b"
      },
      "source": [
        "**B) Time domain Video**\n",
        "\n",
        "Extraction of previously named Time Domain Video Features:\n",
        "- Edge Information (Contour Information)\n",
        "- Edge Orientation Histogram\n",
        "- HUE Information (Dominant colour)\n",
        "- HUE Information - Color Histograms :) <br>"
      ],
      "id": "ha6DlWeXiK0b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Array Information\n",
        "gd_loc_ep1 = \"/content/drive/MyDrive/data/processed_data/episodes/ep1/\"\n",
        "gd_loc_ep2 = \"/content/drive/MyDrive/data/processed_data/episodes/ep2/\"\n",
        "gd_loc_ep3 = \"/content/drive/MyDrive/data/processed_data/episodes/ep3/\"\n",
        "\n",
        "ep_paths = []\n",
        "\n",
        "ep_paths.append(gd_loc_ep1 + \"frame_array_ep1_1.npy\")\n",
        "ep_paths.append(gd_loc_ep1 + \"frame_array_ep1_2.npy\")\n",
        "ep_paths.append(gd_loc_ep1 + \"frame_array_ep1_3.npy\")\n",
        "ep_paths.append(gd_loc_ep1 + \"frame_array_ep1_4.npy\")\n",
        "\n",
        "ep_paths.append(gd_loc_ep2 + \"frame_array_ep2_1.npy\")\n",
        "ep_paths.append(gd_loc_ep2 + \"frame_array_ep2_2.npy\")\n",
        "ep_paths.append(gd_loc_ep2 + \"frame_array_ep2_3.npy\")\n",
        "ep_paths.append(gd_loc_ep2 + \"frame_array_ep2_4.npy\")\n",
        "\n",
        "ep_paths.append(gd_loc_ep3 + \"frame_array_ep3_1.npy\")\n",
        "ep_paths.append(gd_loc_ep3 + \"frame_array_ep3_2.npy\")\n",
        "ep_paths.append(gd_loc_ep3 + \"frame_array_ep3_3.npy\")\n",
        "ep_paths.append(gd_loc_ep3 + \"frame_array_ep3_4.npy\")\n",
        "\n",
        "# Saving Calculated Features to save time in the future\n",
        "video_feature_save_loc = \"/content/drive/MyDrive/data/processed_data/time_video/\""
      ],
      "metadata": {
        "id": "zwrbbK0bOuha"
      },
      "id": "zwrbbK0bOuha",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htsZa2ajiK0d",
        "outputId": "8754189c-c62b-4f82-bc74-5cfa848ca0fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        }
      ],
      "source": [
        "# Edge Information, Number of Edges, Number of Objects in Image\n",
        "# NOTE: The cv2 Canny method uses a Sobel Kernel for Edge Computation\n",
        "feature_vector_objects = []\n",
        "feature_vector_edge_histograms = []\n",
        "feature_vector_contour_areas = []\n",
        "feature_vector_contour_centroids = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  edge_batch = []\n",
        "  for image in ep_batch:\n",
        "\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    image_with_edges = cv2.Canny(gray , 100, 200)\n",
        "    \n",
        "    contours, hierarchy= cv2.findContours(image_with_edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    number_of_objects_in_image = len(contours)\n",
        "    feature_vector_objects.append(number_of_objects_in_image)\n",
        "    \n",
        "    cnt_areas = []\n",
        "    cnt_centroids = [] \n",
        "    for cnt in contours:\n",
        "      M = cv2.moments(cnt)\n",
        "      if M[\"m00\"] != 0:\n",
        "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "      else:\n",
        "      # set values as what you need in the situation\n",
        "        cX, cY = 0, 0\n",
        "\n",
        "      area = cv2.contourArea(cnt)\n",
        "      cnt_areas.append(area)\n",
        "      cnt_centroids.append((cx,cy))\n",
        "\n",
        "    feature_vector_contour_areas.append(cnt_areas)\n",
        "    feature_vector_contour_centroids.append(cnt_centroids)\n",
        "\n",
        "    hist = cv2.calcHist([image_with_edges],[0],None,[16],[0,256])\n",
        "    feature_vector_edge_histograms.append(hist)\n",
        "\n",
        "np.save(video_feature_save_loc + 'countours_feature_vector.npy', feature_vector_objects)\n",
        "np.save(video_feature_save_loc + 'countour_areas_feature_vector.npy', feature_vector_contour_areas)\n",
        "np.save(video_feature_save_loc + 'countour_centroids_feature_vector.npy', feature_vector_contour_centroids)\n",
        "np.save(video_feature_save_loc + 'countour_histograms_feature_vector.npy', feature_vector_edge_histograms)"
      ],
      "id": "htsZa2ajiK0d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge Orientation Histogram\n",
        "\n",
        "# 1. Clalculate gradient\n",
        "# 2. Calculate histogram\n",
        "\n",
        "feature_vector_eoh = [] \n",
        "feature_vector_gradient = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  edge_batch = []\n",
        "  for image in ep_batch:\n",
        "    # First we do the Sobel Gradient\n",
        "    sobelx = cv2.Sobel(image,cv2.CV_64F,1,0,ksize=5)\n",
        "    sobely = cv2.Sobel(image,cv2.CV_64F,0,1,ksize=5)\n",
        "    # And get combined gradient\n",
        "    grad  = np.sqrt(sobelx**2 + sobely**2)\n",
        "    abs_grad = np.absolute(grad)\n",
        "    grad_8u = np.uint8(abs_grad)\n",
        "    # Now we make a nice little histogram\n",
        "    hist = cv2.calcHist([grad_8u],[0],None,[256],[0,256])\n",
        "    \n",
        "    feature_vector_eoh.append(hist)\n",
        "    #feature_vector_gradient.append(grad_8u)\n",
        "\n",
        "np.save(video_feature_save_loc + 'eoh_feature_vector.npy', feature_vector_eoh)\n",
        "#np.save(video_feature_save_loc + 'gradient_feature_vector.npy', feature_vector_gradient)"
      ],
      "metadata": {
        "id": "pBczZ6CWdY8g"
      },
      "id": "pBczZ6CWdY8g",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lJB-ZkdaiK0c"
      },
      "outputs": [],
      "source": [
        " # HUE Information\n",
        " # Dominant Color\n",
        "feature_vector_dominant_color = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  for image in ep_batch:\n",
        "    \n",
        "    hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv_img],[0],None,[256],[0,256])\n",
        "\n",
        "    #Convert histogram to simple list\n",
        "    hist = [val[0] for val in hist]; \n",
        "    #Generate a list of indices\n",
        "    indices = list(range(0, 256));\n",
        "    #Descending sort-by-key with histogram value as key\n",
        "    s = [(x,y) for y,x in sorted(zip(hist,indices), reverse=True)]\n",
        "    #Index of highest peak in histogram\n",
        "    index_of_max = s[0][0];\n",
        "    #Index of second highest peak in histogram\n",
        "    index_of_max2 = s[1][0];\n",
        "\n",
        "    feature_vector_dominant_color.append((hist[index_of_max], hist[index_of_max2]))\n",
        "    \n",
        "np.save(video_feature_save_loc + 'feature_vector_dominant_color.npy', feature_vector_dominant_color) "
      ],
      "id": "lJB-ZkdaiK0c"
    },
    {
      "cell_type": "code",
      "source": [
        "# HUE Information\n",
        "# Color Histograms - Just to keep the proffesor interested :)\n",
        "feature_vector_color_histogram = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  for image in ep_batch:\n",
        "    hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv_img],[0],None,[256],[0,256])\n",
        "\n",
        "    feature_vector_color_histogram.append(hist)\n",
        "\n",
        "np.save(video_feature_save_loc + 'feature_vector_color_histogram.npy', feature_vector_color_histogram) "
      ],
      "metadata": {
        "id": "p2jGcMJbyBin"
      },
      "id": "p2jGcMJbyBin",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) **Classification** (Experimentation)\n",
        "\n",
        "In this section we will use the calculated Audio and Visual features for classification.\n",
        "<br>\n",
        "I have decided to use the K-Nearest-Neighbours Classifier, as it seems to be very popular for these kinds of tasks"
      ],
      "metadata": {
        "id": "8WhxwFtElN71"
      },
      "id": "8WhxwFtElN71"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Vector Locations\n",
        "\n",
        "audio_time_save_loc = \"/content/drive/MyDrive/data/processed_data/time_audio/\"\n",
        "video_time_save_loc = \"/content/drive/MyDrive/data/processed_data/time_video/\""
      ],
      "metadata": {
        "id": "6IUkmqYwZjdQ"
      },
      "id": "6IUkmqYwZjdQ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting Audio for Stadtler and Waldorf\n",
        "\n",
        "y_train = statler_waldorf_audio_train\n",
        "y_test = statler_waldorf_audio_test\n",
        "\n",
        "# RMS\n",
        "\n",
        "# Loudness LOAD-UP\n",
        "rms_ep1_all = np.array(np.load(audio_time_save_loc + 'rms_ep1.npy'))\n",
        "rms_ep2_all = np.array(np.load(audio_time_save_loc + 'rms_ep2.npy'))\n",
        "rms_ep3_all = np.array(np.load(audio_time_save_loc + 'rms_ep3.npy'))\n",
        "\n",
        "rms_train = np.hstack((rms_ep1_all, rms_ep2_all)) \n",
        "\n",
        "rms_test = np.asarray(rms_ep3_all)\n",
        "\n",
        "\n",
        "\n",
        "model_audio_rms = KNeighborsClassifier(n_neighbors = 5)\n",
        "model_audio_rms.fit(rms_train, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "outputs_rms = model_audio.predict(rms_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for (output, label) in zip(outputs_rms, y_test):\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "h51iTvyMlYr1",
        "outputId": "8bad5382-c1f4-4f1c-90c5-2002afd30b7a"
      },
      "id": "h51iTvyMlYr1",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-b98d6e51995d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrms_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrms_ep1_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrms_ep2_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrms_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrms_ep3_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 66618 and the array at index 1 has size 66659"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting Audio for Stadtler and Waldorf\n",
        "y_train = statler_waldorf_audio_train\n",
        "y_test = statler_waldorf_audio_test\n",
        "\n",
        "# Fundamental Frequencies\n",
        "# Fundamental ZCR LOAD-UP\n",
        "zcr_ep1 = np.load(audio_time_save_loc + 'zcr_ep1.npy')\n",
        "zcr_ep2 = np.load(audio_time_save_loc + 'zcr_ep2.npy')\n",
        "zcr_ep3 = np.load(audio_time_save_loc + 'zcr_ep3.npy')\n",
        "\n",
        "zcr_ep1 = np.array(zcr_ep1)\n",
        "zcr_ep2 = np.array(zcr_ep2)\n",
        "zcr_ep3 = np.array(zcr_ep3)\n",
        "\n",
        "zcr_train = np.hstack((zcr_ep1, zcr_ep2))\n",
        "zcr_test = zcr_ep3\n",
        "\n",
        "print(zcr_train.shape)\n",
        "\n",
        "# Fundamental YIN LOAD-UP\n",
        "f0s_ep1 = np.load(time_audio_loc + 'yin_f0s_ep1.npy')\n",
        "f0s_ep2 = np.load(time_audio_loc + 'yin_f0s_ep2.npy')\n",
        "f0s_ep3 = np.load(time_audio_loc + 'yin_f0s_ep3.npy')\n",
        "\n",
        "f0s_ep1 = np.array(f0s_ep1)\n",
        "f0s_ep2 = np.array(f0s_ep2)\n",
        "f0s_ep3 = np.array(f0s_ep3)\n",
        "\n",
        "f0s_train = np.hstack((f0s_ep1, f0s_ep2))\n",
        "f0s_test = f0s_ep3\n",
        "\n",
        "feature_vector_final_train = np.vstack((zcr_train,f0s_train))\n",
        "\n",
        "feature_vector_final_test = np.vstack((zcr_test,f0s_test))\n",
        "\n",
        "\n",
        "model_audio_ff = KNeighborsClassifier(n_neighbors = 5)\n",
        "model_audio_ff.fit(feature_vector_final_train, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "outputs_ff = model_audio.predict(feature_vector_final_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for (output, label) in zip(outputs_ff, y_test):\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "55iGHCpd18fv",
        "outputId": "bdc944e5-7cb4-494d-b9c2-5562b0d06b25"
      },
      "id": "55iGHCpd18fv",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 133277)\n",
            "(2, 133277)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-3a7e498e3a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel_audio_ff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel_audio_ff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_final_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m## TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    331\u001b[0m         raise ValueError(\n\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 77387]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# Dominant Color\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "# Color Dominance LOAD-UP\n",
        "feature_vector_dominant_color = np.load(video_time_save_loc + 'feature_vector_dominant_color.npy') \n",
        "\n",
        "feature_vector_train = feature_vector_dominant_color[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_dominant_color[range(train_size, len(feature_vector_dominant_color))][:]\n",
        "\n",
        "model_cd = KNeighborsClassifier(n_neighbors = 3)\n",
        "model_cd.fit(feature_vector_train, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "outputs = model_cd.predict(feature_vector_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDDClLDfJK4C",
        "outputId": "60d1c923-b572-4fe2-b953-4dab3e205ef6"
      },
      "id": "sDDClLDfJK4C",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(77387, 2)\n",
            "0.40325670498084293\n",
            "0.09576888080072793\n",
            "0.1547794117647059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prec = 0.41214549938347717\n",
        "recall = 0.05963425512934879\n",
        "fmeasure = 0.10419264339152119\n",
        "\n",
        "Conclusion: Dominant Color Works Horribly"
      ],
      "metadata": {
        "id": "9qZghkhmJJA5"
      },
      "id": "9qZghkhmJJA5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "#feature_vector_color_histogram\n",
        "\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "# Color Histogram LOAD-UP\n",
        "feature_vector_color_histogram = np.load(video_time_save_loc + 'feature_vector_color_histogram.npy') \n",
        "\n",
        "feature_vector_train = feature_vector_color_histogram[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_color_histogram[range(train_size, len(feature_vector_color_histogram))][:]\n",
        "\n",
        "print(feature_vector_train.shape)\n",
        "\n",
        "nsamples, nx, ny = feature_vector_train.shape\n",
        "d2_train_dataset = feature_vector_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "model_cd = KNeighborsClassifier(n_neighbors = 3)\n",
        "model_cd.fit(d2_train_dataset, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "nsamples, nx, ny = feature_vector_test.shape\n",
        "d2_test_dataset = feature_vector_test.reshape((nsamples,nx*ny))\n",
        "\n",
        "outputs = model_cd.predict(d2_test_dataset)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZiwnzGK5tab",
        "outputId": "4aef4dca-d0a2-43ec-a954-915e310ad4e2"
      },
      "id": "SZiwnzGK5tab",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(77387, 256, 1)\n",
            "0.45979614949037373\n",
            "0.07152294547696644\n",
            "0.12378992301242472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision:0.45979614949037373 Recall:0.07152294547696644 F-measure:0.12378992301242472\n",
        "Color histogram, also horrible with KNN"
      ],
      "metadata": {
        "id": "qF23pYSrMl1p"
      },
      "id": "qF23pYSrMl1p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# Edge/Contour Information\n",
        "\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "# Edge Information LOAD-UP\n",
        "feature_vector_objects = np.array(np.load(video_time_save_loc + 'countours_feature_vector.npy'))\n",
        "feature_vector_countour_areas = np.array(np.load(video_time_save_loc + 'countour_areas_feature_vector.npy', allow_pickle = True))\n",
        "feature_vector_countour_centroids = np.array(np.load(video_time_save_loc + 'countour_centroids_feature_vector.npy', allow_pickle = True))\n",
        "\n",
        "feature_vector_final = np.dstack((feature_vector_objects, feature_vector_countour_areas))\n",
        "#feature_vector_final = feature_vector_final.transpose(1,2,0)\n",
        "\n",
        "\n",
        "print(feature_vector_final.shape)\n",
        "del feature_vector_objects\n",
        "del feature_vector_countour_areas\n",
        "del feature_vector_countour_centroids\n",
        "\n",
        "feature_vector_train = feature_vector_final[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_final[range(train_size, len(feature_vector_final))][:]\n",
        "\n",
        "nsamples, nx, ny = feature_vector_train.shape\n",
        "d2_train_dataset = feature_vector_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "model_cd = KNeighborsClassifier(n_neighbors = 3)\n",
        "model_cd.fit(d2_train_dataset, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "\n",
        "outputs = model_cd.predict(feature_vector_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VBuPV8C5kXp",
        "outputId": "3d700a43-6446-4daf-9f2a-3a658ed46aac"
      },
      "id": "4VBuPV8C5kXp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 115888)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kH4un7YyMibn"
      },
      "id": "kH4un7YyMibn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# Edge Orientation Histograms\n",
        "\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "# Edge Orientation HISTOGRAM LOAD-UP\n",
        "feature_vector_eoh = np.load(video_time_save_loc + 'eoh_feature_vector.npy')\n",
        "\n",
        "feature_vector_train = feature_vector_eoh[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_eoh[range(train_size, len(feature_vector_eoh))][:]\n",
        "\n",
        "nsamples, nx, ny = feature_vector_train.shape\n",
        "d2_train_dataset = feature_vector_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "model_cd = KNeighborsClassifier(n_neighbors = 3)\n",
        "model_cd.fit(d2_train_dataset, y_train)\n",
        "\n",
        "## TEST\n",
        "\n",
        "\n",
        "outputs = model_cd.predict(feature_vector_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)"
      ],
      "metadata": {
        "id": "YVdQQd5x5fSY"
      },
      "id": "YVdQQd5x5fSY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video + Audio for Kermit\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors = 5)\n",
        "neigh.fit(X, y)\n",
        "\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  "
      ],
      "metadata": {
        "id": "MWhWP245JSeN"
      },
      "id": "MWhWP245JSeN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Evaluation and Visualisation"
      ],
      "metadata": {
        "id": "uxQGN2UXlvsB"
      },
      "id": "uxQGN2UXlvsB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Comment"
      ],
      "metadata": {
        "id": "Uq6cEWRply7G"
      },
      "id": "Uq6cEWRply7G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXCf2ItxiK0e"
      },
      "source": [
        "# Similarity Modelling 2\n",
        "\n",
        "The features relevant for Similarity Modelling 2: <br>\n",
        "\n",
        "\n",
        "**Audio:**\n",
        "\n",
        "Frequency domain features:\n",
        "- The mel frequency cepstral coefficients (MFCCs)\n",
        "- Spectral peaks\n",
        "\n",
        "**Video:**\n",
        "\n",
        "Frequency domain features:\n",
        "- Luminance + Discrete Cosine Transform <br>\n",
        "\n",
        "Local methods:\n",
        "- SIFT - Scale Invariant Feature Transformation\n",
        "  (Bag of Visual Features Variant)\n",
        "- HOG - Histogram of Oriented Gradients\n",
        "- Shi-Tomasi Corner Detection\n"
      ],
      "id": "mXCf2ItxiK0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFWUO4PjiK0e"
      },
      "source": [
        "**Frequency Domain Feature Understanding**\n",
        "\n",
        "Extracting the audio features.\n",
        "\n",
        "I put in some effort to understand multiple audio signal processing methods.\n",
        "Including Spectrogram creations, DFTs, Beat Detections, OnSet Detections, the YIN method "
      ],
      "id": "gFWUO4PjiK0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6vAHLG0iK0f"
      },
      "source": [
        "**A) Audio Features (Frequency domain)**\n",
        "\n",
        "Most of the frequency domain calculations refer to a spectrogram\n",
        "Some time to explain what the spectrorgram is:\n",
        "\n",
        "- We can calculate a Fourier Transform for a signal of a certain size (e.g. 2048 frames)\n",
        "- If we do this with a sliding window over the whole signal - we get a spectrogram\n",
        "\n",
        "Extraction of previously named Frequency Domain Audio Features: <br>\n",
        "- The mel frequency cepstral coefficients (MFCCs)\n",
        "- Spectral peaks <br>"
      ],
      "id": "y6vAHLG0iK0f"
    },
    {
      "cell_type": "code",
      "source": [
        "freq_audio_loc = \"/content/drive/MyDrive/data/processed_data/frequency_audio/\""
      ],
      "metadata": {
        "id": "f-YP6sm8K5FM"
      },
      "id": "f-YP6sm8K5FM",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CSmOObxoiK0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e0b7aa-43a8-4e73-9b76-34a6c638f02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "# MFCCs \n",
        "# Calculating the mel log spectrogram, and then doing the factor analysis\n",
        "signal_ep1, sr  = librosa.load(audio_ep1)\n",
        "signal_ep2, sr  = librosa.load(audio_ep2)\n",
        "signal_ep3, sr  = librosa.load(audio_ep3)\n",
        "\n",
        "mfcc_ep1 = librosa.feature.mfcc(y = signal_ep1, sr=sr)\n",
        "mfcc_ep2 = librosa.feature.mfcc(y = signal_ep2, sr=sr)\n",
        "mfcc_ep3 = librosa.feature.mfcc(y = signal_ep3, sr=sr)\n",
        "\n",
        "np.save(freq_audio_loc + 'mfcc_ep1.npy', mfcc_ep1)\n",
        "np.save(freq_audio_loc + 'mfcc_ep2.npy', mfcc_ep2)\n",
        "np.save(freq_audio_loc + 'mfcc_ep3.npy', mfcc_ep3)\n",
        "\n",
        "print(len(mfcc_ep1))\n",
        "del mfcc_ep1\n",
        "del mfcc_ep2\n",
        "del mfcc_ep3"
      ],
      "id": "CSmOObxoiK0g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLNSWE11iK0g"
      },
      "outputs": [],
      "source": [
        "# Spectral peaks method\n",
        "# First we calculate spectrograms for each signal\n",
        "# Then we calculate energy contrasts\n",
        "signal_ep1, sr  = librosa.load(audio_ep1)\n",
        "signal_ep2, sr  = librosa.load(audio_ep2)\n",
        "signal_ep3, sr  = librosa.load(audio_ep3)\n",
        "\n",
        "spectrogram_ep1 = np.abs(librosa.stft(signal_ep1))\n",
        "contrast_ep1 = librosa.feature.spectral_contrast(S=spectrogram_ep1, sr=sr)\n",
        "np.save(freq_audio_loc + 'contrast_ep1.npy', contrast_ep1)\n",
        "print(len(contrast_ep1))\n",
        "del contrast_ep1\n",
        "del spectrogram_ep1\n",
        "\n",
        "spectrogram_ep2 = np.abs(librosa.stft(signal_ep2))\n",
        "contrast_ep2 = librosa.feature.spectral_contrast(S=spectrogram_ep2, sr=sr)\n",
        "np.save(freq_audio_loc + 'contrast_ep2.npy', contrast_ep2)\n",
        "del contrast_ep2\n",
        "del spectrogram_ep2\n",
        "\n",
        "spectrogram_ep3 = np.abs(librosa.stft(signal_ep3))\n",
        "contrast_ep3 = librosa.feature.spectral_contrast(S=spectrogram_ep3, sr=sr)\n",
        "np.save(freq_audio_loc + 'contrast_ep3.npy', contrast_ep3)\n",
        "del contrast_ep3\n",
        "del spectrogram_ep3\n",
        "\n",
        "# From energy contrasts we can find the maximum values"
      ],
      "id": "kLNSWE11iK0g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPSuOoCWiK0h"
      },
      "source": [
        "**B) Video Features (Frequency Domain)**\n",
        "\n",
        "Extraction of previously named Frequency Domain Video Features: <br>\n",
        "\n",
        "- Luminance + Discrete Cosine Transform"
      ],
      "id": "lPSuOoCWiK0h"
    },
    {
      "cell_type": "code",
      "source": [
        "video_freq_save_loc = \"/content/drive/MyDrive/data/processed_data/frequency_video/\""
      ],
      "metadata": {
        "id": "JBh1uArOP2O1"
      },
      "id": "JBh1uArOP2O1",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EC8u-wvniK0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693f77ff-5c4e-48f7-9364-1ca44f4c7a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(272, 360)\n",
            "(272, 360)\n"
          ]
        }
      ],
      "source": [
        "# Luminance + Discrete Cosine Transform\n",
        "dct_feature_vector = []\n",
        "comp = 0\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  if(comp >= 20):\n",
        "    break\n",
        "\n",
        "  edge_batch = []\n",
        "  for image in ep_batch:\n",
        "    comp += 1\n",
        "    # calculate the Discrete Cosine Transform\n",
        "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    dct_img = scipy.fftpack.dct(img)\n",
        "\n",
        "    if(comp == 1):\n",
        "      print(dct_img.shape)\n",
        "\n",
        "    if(comp == 20):\n",
        "      print(dct_img.shape)\n",
        "      break\n",
        "\n",
        "    dct_feature_vector.append(dct[range(0,28)][range(0:36)])\n",
        "\n",
        "np.save(video_freq_save_loc + 'feature_vector_dct.npy', dct_feature_vector)"
      ],
      "id": "EC8u-wvniK0i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnsVlbp0iK0j"
      },
      "source": [
        "**C) Video Features (Local methods)**"
      ],
      "id": "gnsVlbp0iK0j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSnxA8jGiK0j"
      },
      "source": [
        "Local methods:\n",
        "- SIFT - Scale Invariant Feature Transformation\n",
        "  (Bag of Visual Features Variant)\n",
        "- HOG - Histogram of Oriented Gradients\n",
        "- Shi-Tomasi Corner Detection"
      ],
      "id": "eSnxA8jGiK0j"
    },
    {
      "cell_type": "code",
      "source": [
        "video_feature_save_loc = \"/content/drive/MyDrive/data/processed_data/local_video/\""
      ],
      "metadata": {
        "id": "pKLTklwVBxoQ"
      },
      "id": "pKLTklwVBxoQ",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of Oriented Gradients (HOG)\n",
        "# We have to rescale, otherwise HOG becomes far too large\n",
        "vidcap = cv2.VideoCapture(path_ep1)\n",
        "(success,image) = vidcap.read()\n",
        "scale_percent = 30 # percent of original size\n",
        "width = int(image.shape[1] * scale_percent / 100)\n",
        "height = int(image.shape[0] * scale_percent / 100)\n",
        "dim = (width, height)\n",
        "\n",
        "feature_vector_hog = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        " \n",
        "\n",
        "  for image in ep_batch:\n",
        "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "    gray = cv2.cvtColor(resized,cv2.COLOR_BGR2GRAY)\n",
        "    H = feature.hog(gray, orientations=9, pixels_per_cell=(10, 10),\n",
        "\t    cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\")\n",
        "    feature_vector_hog.append(H)\n",
        "\n",
        "np.save(video_feature_save_loc + 'feature_vector_hog.npy', feature_vector_hog)"
      ],
      "metadata": {
        "id": "keWeSJ71dSWx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "1cbe308c-e7c0-4fd2-8df3-f9bdd4c4df91"
      },
      "id": "keWeSJ71dSWx",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a6644ff9b755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mep_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mep_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PIoCwNtwiK0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "30f984e3-b69b-4867-ded0-0cd4f1e0211d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7150ae6e092e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#np.save(video_feature_save_loc + 'SIFT_totals.npy', total_SIFT_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtotal_SIFT_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_feature_save_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'SIFT_totals.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_SIFT_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtotal_SIFT_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(obs, k_or_guess, iter, thresh, check_finite)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# the initial code book is randomly selected from observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mbest_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36m_kmeans\u001b[0;34m(obs, guess, thresh)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# compute membership and distances between obs and code_book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mobs_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_book\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mprev_avg_dists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# recalc code_book as centroids of associated obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36mvq\u001b[0;34m(obs, code_book, check_finite)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_code_book\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpy_vq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_book\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# SIFT\n",
        "# We use cv2 for SIFT\n",
        "# The way it works:\n",
        "# 1. Finds IPs - interest points (Exist on multiple coarsness levels)\n",
        "# 2. Save IP neighbourhoods: big neighbourhood -> more important point\n",
        "# 3. Calculates Gradient Histrogram\n",
        "# We then use SIFT features to create histograms\n",
        "n_samples = 20\n",
        "total_SIFT_features = []\n",
        "vocab_size = 200\n",
        "feats = []\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  for image in ep_batch:\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    kp, des = sift.detectAndCompute(gray,None)\n",
        "\n",
        "    if(des is None):\n",
        "      for i in range(0, n_samples):\n",
        "        total_SIFT_features.append(np.zeros(128))\n",
        "    else:\n",
        "      des_samples = des[np.random.randint(des.shape[0], size = n_samples)]\n",
        "\n",
        "      for i in range(0, n_samples):\n",
        "        total_SIFT_features.append(des_samples[i][:])\n",
        "\n",
        "#np.save(video_feature_save_loc + 'SIFT_totals.npy', total_SIFT_features)\n",
        "#total_SIFT_features = np.load(video_feature_save_loc + 'SIFT_totals.npy')\n",
        "vocab = scipy.cluster.vq.kmeans(total_SIFT_features, vocab_size)\n",
        "\n",
        "del total_SIFT_features\n",
        "\n",
        "np.save(video_feature_save_loc + 'SIFT_vocab.npy', vocab)\n",
        "\n",
        "# Calculating SIFT feature Histograms\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  for image in ep_batch:\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    kp, des = sift.detectAndCompute(gray,None)\n",
        "\n",
        "    dist = scipy.spatial.distance.cdist(des, vocab, \"euclidean\")\n",
        "\n",
        "    bin_assignment = np.argmin(dist, axis = 1)\n",
        "\n",
        "    image_features = np.zeros(200)\n",
        "\n",
        "    for bin in bin_assignment:\n",
        "      image_feats[id] += 1\n",
        "\n",
        "    feats.append(image_feats)\n",
        "\n",
        "feats = np.asarray(feats)\n",
        "feats_norm_div = np.linalg.norm(feats, axis = 1)\n",
        "\n",
        "for i in range(0, feats.shape[0]):\n",
        "  feats[i] = feats[i] / feats_norm_div[i]  \n",
        "\n",
        "np.save(video_feature_save_loc + 'feature_vector_SIFT_bag.npy', feats)\n"
      ],
      "id": "PIoCwNtwiK0j"
    },
    {
      "cell_type": "code",
      "source": [
        "# Shi-Tomasi Corner Detector\n",
        "\n",
        "feature_vector_shi_tomasi = []\n",
        "\n",
        "for ep_path in ep_paths:\n",
        "  ep_batch = np.load(ep_path)\n",
        "\n",
        "  for image in ep_batch:\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    corners = cv2.goodFeaturesToTrack(gray,25,0.01,10)\n",
        "    \n",
        "    if corners is not None:\n",
        "      for i in corners:\n",
        "        x,y = i.ravel()\n",
        "        image = cv2.circle(image,(x,y),3,255,-1)\n",
        "    \n",
        "    hist = cv2.calcHist([image],[0],None,[256],[0,256])\n",
        "\n",
        "    feature_vector_shi_tomasi.append(hist)\n",
        "\n",
        "np.save(video_feature_save_loc + 'feature_vector_shi_tomasi.npy', feature_vector_shi_tomasi)"
      ],
      "metadata": {
        "id": "sGYEgbTtv0cv"
      },
      "id": "sGYEgbTtv0cv",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) **Classification** (and Experimentation)"
      ],
      "metadata": {
        "id": "kJQL2wv0lf_U"
      },
      "id": "kJQL2wv0lf_U"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Vector Locations\n",
        "\n",
        "audio_time_save_loc = \"/content/drive/MyDrive/data/processed_data/time_audio/\"\n",
        "video_time_save_loc = \"/content/drive/MyDrive/data/processed_data/time_video/\"\n",
        "audio_freq_save_loc = \"/content/drive/MyDrive/data/processed_data/frequency_audio/\"\n",
        "video_local_save_loc = \"/content/drive/MyDrive/data/processed_data/local_video/\"\n",
        "video_freq_save_loc = \"/content/drive/MyDrive/data/processed_data/frequency_video/\""
      ],
      "metadata": {
        "id": "qGOx1X8uOs76"
      },
      "id": "qGOx1X8uOs76",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MFCC LOAD-UP\n",
        "mfcc_ep1 = np.load(audio_freq_save_loc + 'mfcc_ep1.npy')\n",
        "mfcc_ep2 = np.load(audio_freq_save_loc + 'mfcc_ep2.npy')\n",
        "mfcc_ep3 = np.load(audio_freq_save_loc + 'mfcc_ep3.npy')"
      ],
      "metadata": {
        "id": "xlaS_2xbXxi0"
      },
      "id": "xlaS_2xbXxi0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPECTRAL CONTRAS LOAD-UP\n",
        "contrast_ep1 = np.load(audio_freq_save_loc + 'contrast_ep1.npy')\n",
        "contrast_ep2 = np.load(audio_freq_save_loc + 'contrast_ep2.npy')\n",
        "contrast_ep3 = np.load(audio_freq_save_loc + 'contrast_ep3.npy')"
      ],
      "metadata": {
        "id": "0tDKRwt-X8mn"
      },
      "id": "0tDKRwt-X8mn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Luminance + DCT LOAD-UP\n",
        "dct_feature_vector = np.load(video_freq_save_loc + 'feature_vector_dct.npy')"
      ],
      "metadata": {
        "id": "7rjtkLuvYLwo"
      },
      "id": "7rjtkLuvYLwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HOG LOAD-UP\n",
        "feature_vector_hog = np.save(video_local_save_loc + 'feature_vector_hog.npy')"
      ],
      "metadata": {
        "id": "hKuRyfVYYnGK"
      },
      "id": "hKuRyfVYYnGK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shi Tomasi LOAD-UP\n",
        "feature_vector_shi_tomasi = np.save(video_local_save_loc + 'feature_vector_shi_tomasi.npy')"
      ],
      "metadata": {
        "id": "-m8lugDnY9Dl"
      },
      "id": "-m8lugDnY9Dl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIFT Bag LOAD-UP\n",
        "feature_vector_SIFT_bag = np.save(video_local_save_loc + 'feature_vector_SIFT_bag.npy')"
      ],
      "metadata": {
        "id": "3s-6G7bwZI31"
      },
      "id": "3s-6G7bwZI31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Feature Classification for Stadter and Waldorf\n",
        "# MFCC Classifier\n",
        "y_train = statler_waldorf_audio_train\n",
        "y_test = statler_waldorf_audio_test\n",
        "\n",
        "mfcc_ep1 \n",
        "mfcc_ep2 \n",
        "mfcc_ep3\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "clf.fit(X, y_test)\n",
        "\n",
        "\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for (output, label) in zip(predictions, y_test):\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  "
      ],
      "metadata": {
        "id": "5ZSliLyNlfm1"
      },
      "id": "5ZSliLyNlfm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Feature Classification for Stadter and Waldorf\n",
        "# Spectral Peaks\n",
        "\n",
        "contrast_ep1 \n",
        "contrast_ep2 \n",
        "contrast_ep3\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "clf.fit(X, y_test)\n",
        "\n",
        "\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for (output, label) in zip(predictions, y_test):\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  "
      ],
      "metadata": {
        "id": "RUQ5w4pn3TQ1"
      },
      "id": "RUQ5w4pn3TQ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# HOG Classification\n",
        "\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "# HOG LOAD-UP\n",
        "feature_vector_hog = np.load(video_local_save_loc + 'feature_vector_hog.npy')\n",
        "\n",
        "print(feature_vector_hog.shape)\n",
        "\n",
        "feature_vector_train = feature_vector_hog[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_hog[range(train_size, len(feature_vector_hog))][:]\n",
        "\n",
        "print(feature_vector_train.shape)\n",
        "\n",
        "nsamples, nx, ny = feature_vector_train.shape\n",
        "d2_train_dataset = feature_vector_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "clf.fit(feature_vector_train, y_train)\n",
        "\n",
        "#TEST\n",
        "\n",
        "nsamples, nx, ny = feature_vector_test.shape\n",
        "d2_test_dataset = feature_vector_test.reshape((nsamples,nx*ny))\n",
        "\n",
        "outputs = clf.predict(feature_vector_test)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "fJroLb4JJidS",
        "outputId": "bfa25f38-f5e1-4678-d87f-58e6505e4a51"
      },
      "id": "fJroLb4JJidS",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8497, 10800)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-24f81a77be9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_hog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfeature_vector_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector_hog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfeature_vector_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector_hog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_hog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 8497 is out of bounds for axis 0 with size 8497"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# Shi-Tomasi Corner Detection\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_size = len(y_train)\n",
        "\n",
        "feature_vector_shi_tomasi = np.load(video_local_save_loc + 'feature_vector_shi_tomasi.npy')\n",
        "\n",
        "print(feature_vector_shi_tomasi.shape)\n",
        "\n",
        "feature_vector_train = feature_vector_shi_tomasi[range(0, train_size)][:]\n",
        "feature_vector_test = feature_vector_shi_tomasi[range(train_size, len(feature_vector_shi_tomasi))][:]\n",
        "\n",
        "print(feature_vector_train.shape)\n",
        "\n",
        "nsamples, nx, ny = feature_vector_train.shape\n",
        "d2_train_dataset = feature_vector_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "clf.fit(d2_train_dataset, y_train)\n",
        "\n",
        "#TEST\n",
        "\n",
        "nsamples, nx, ny = feature_vector_test.shape\n",
        "d2_test_dataset = feature_vector_test.reshape((nsamples,nx*ny))\n",
        "\n",
        "outputs = clf.predict(d2_test_dataset)\n",
        "\n",
        "# Calculate Precision, Recall and F-measure\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "count = 0\n",
        "for (output, label) in zip(outputs, y_test):\n",
        "  count += 1\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  \n",
        "\n",
        "precision = tp/(tp + fp)\n",
        "recall = tp/(tp+fn)\n",
        "fmeasure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(fmeasure)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDqFz9FF3uSy",
        "outputId": "cc8a0aff-2716-46af-9de4-7b323bf1e9d3"
      },
      "id": "QDqFz9FF3uSy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(115888, 256, 1)\n",
            "(77387, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video for Kermit\n",
        "# DCT + Luminance\n",
        "y_train = kermit_train\n",
        "y_test = kermit_test\n",
        "train_len = len(y_train)\n",
        "\n",
        "print(dct_feature_vector.shape)\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "clf.fit(X, y_test)\n",
        "\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for (output, label) in zip(predictions, y_test):\n",
        "  if(output == 1 and label == 1):\n",
        "    tp += 1\n",
        "  if(output == 1 and label == 0):\n",
        "    fp += 1\n",
        "  if(output == 0 and label == 0):\n",
        "    fn += 1  "
      ],
      "metadata": {
        "id": "D5j_DFiw3uoN"
      },
      "id": "D5j_DFiw3uoN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Evaluation and Visualisation"
      ],
      "metadata": {
        "id": "dY38T-xqlmqw"
      },
      "id": "dY38T-xqlmqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Comment\n",
        "\n",
        "SIFT:\n",
        "- In the end the Bag of Visual Features Algorithm using SIFT proved\n",
        "to take too long to compute for a dataset of this size, however\n",
        "the key ideas were learned and this Feature Descriptor can prove to\n",
        "be valuable on small Image datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "EResX6BllodY"
      },
      "id": "EResX6BllodY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "similarity_modelling_fe_part.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}